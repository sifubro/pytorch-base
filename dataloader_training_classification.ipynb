{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e4515c2-8ef8-4aa8-89af-83ebdf9bab96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import random\n",
    "import numpy as np\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "import torch\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b31ccc5-cbea-4a32-a4a3-ed1a892dec22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x243556f76d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set the seed\n",
    "SEED = 1234\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "#tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e81e1a-5378-4464-bbb6-54c1790cc586",
   "metadata": {},
   "source": [
    "### csv with filenames and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1912d570-23d4-4221-b0cb-192f15a4d742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = glob.glob(\"D://DATASETS/DogsVsCats/train-val-test/*.jpg\")\n",
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ea7e89e-2212-4339-a102-a2132e236004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D://DATASETS/DogsVsCats/train-val-test\\cat.0.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D://DATASETS/DogsVsCats/train-val-test\\cat.1.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D://DATASETS/DogsVsCats/train-val-test\\cat.10.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filename  label\n",
       "0   D://DATASETS/DogsVsCats/train-val-test\\cat.0.jpg      0\n",
       "1   D://DATASETS/DogsVsCats/train-val-test\\cat.1.jpg      0\n",
       "2  D://DATASETS/DogsVsCats/train-val-test\\cat.10.jpg      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [1 if \"dog\" in fname else 0 for fname in images]\n",
    "full_df = pd.DataFrame()\n",
    "full_df[\"filename\"] = images\n",
    "full_df[\"label\"] = labels\n",
    "full_df[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b7bade-1933-4f32-8602-3370545c95a1",
   "metadata": {},
   "source": [
    "### Shuffle dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9a4166a-ebd4-493b-8ac7-06db0286d11b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5262</td>\n",
       "      <td>D://DATASETS/DogsVsCats/train-val-test\\cat.348...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22764</td>\n",
       "      <td>D://DATASETS/DogsVsCats/train-val-test\\dog.798...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2633</td>\n",
       "      <td>D://DATASETS/DogsVsCats/train-val-test\\cat.123...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22512</td>\n",
       "      <td>D://DATASETS/DogsVsCats/train-val-test\\dog.776...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19404</td>\n",
       "      <td>D://DATASETS/DogsVsCats/train-val-test\\dog.496...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                           filename  label\n",
       "0   5262  D://DATASETS/DogsVsCats/train-val-test\\cat.348...      0\n",
       "1  22764  D://DATASETS/DogsVsCats/train-val-test\\dog.798...      1\n",
       "2   2633  D://DATASETS/DogsVsCats/train-val-test\\cat.123...      0\n",
       "3  22512  D://DATASETS/DogsVsCats/train-val-test\\dog.776...      1\n",
       "4  19404  D://DATASETS/DogsVsCats/train-val-test\\dog.496...      1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df = full_df.sample(frac=1).reset_index()\n",
    "full_df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8a59721-7309-4112-ae2d-1025d1fcbafd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12500"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.label.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09a0293d-519c-43b6-8e24-62a607a47c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "dogs_df = full_df[full_df.label==1].sample(frac=1).reset_index(drop=True)\n",
    "cats_df = full_df[full_df.label==0].sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "dogs = {\"train\":None, \"test\": None, \"valid\":None}\n",
    "cats = {\"train\":None, \"test\": None, \"valid\":None}\n",
    "\n",
    "dogs[\"train\"], dogs[\"test\"], dogs[\"valid\"] = np.split(dogs_df, [int(0.8*len(dogs_df)), int(0.9*len(dogs_df))])\n",
    "cats[\"train\"], cats[\"test\"], cats[\"valid\"] = np.split(cats_df, [int(0.8*len(cats_df)), int(0.9*len(cats_df))])\n",
    "\n",
    "train_df = pd.concat([dogs[\"train\"], cats[\"train\"]]).sample(frac=1).reset_index(drop=True)\n",
    "test_df = pd.concat([dogs[\"test\"], cats[\"test\"]]).sample(frac=1).reset_index(drop=True)\n",
    "valid_df = pd.concat([dogs[\"valid\"], cats[\"valid\"]]).sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3adc6af5-0d2f-477c-8e48-9ac06c6ae15d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 2500, 2500)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df), len(test_df), len(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad8d69e8-79d7-4e4a-aee9-39ff1ca45924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1550</td>\n",
       "      <td>D://DATASETS/DogsVsCats/train-val-test\\cat.113...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19077</td>\n",
       "      <td>D://DATASETS/DogsVsCats/train-val-test\\dog.466...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5690</td>\n",
       "      <td>D://DATASETS/DogsVsCats/train-val-test\\cat.387...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                           filename  label\n",
       "0   1550  D://DATASETS/DogsVsCats/train-val-test\\cat.113...      0\n",
       "1  19077  D://DATASETS/DogsVsCats/train-val-test\\dog.466...      1\n",
       "2   5690  D://DATASETS/DogsVsCats/train-val-test\\cat.387...      0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "308029a0-468d-4602-a07d-82172031c0de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.label.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc93def-fd91-4ec1-b6c6-9bc5bb80c40b",
   "metadata": {},
   "source": [
    "### `Dataset` stores the samples and their corresponding labels, and `DataLoader` wraps an iterable around the Dataset to enable easy access to the samples.\n",
    "\n",
    "#### https://pytorch.org/docs/stable/data.html#data-loading-order-and-sampler\n",
    "#### https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset\n",
    "\n",
    "### TODO: Sampler\n",
    "\n",
    "#### https://pytorch.org/docs/stable/data.html#torch.utils.data.Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f025fa26-46b7-47ba-b478-9a7a4e7f001a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    '''\n",
    "    https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "    '''\n",
    "    def __init__(self, train_df, transform=None, target_transform=None):\n",
    "        self.train_df = train_df\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = train_df.filename[idx]\n",
    "        # read JPEG or PNG image from filepath  --> output (Tensor[image_channels, image_height, image_width])\n",
    "        image =  read_image(img_path)\n",
    "        label =  train_df.label[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            # These are transformation single level\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "            \n",
    "        return {'image': image, 'label': label}  #image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdeb0846-3835-43be-a793-a15bdfa1e5fa",
   "metadata": {},
   "source": [
    "### https://pytorch.org/tutorials/beginner/data_loading_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5ae26bb-9dc8-4b5a-972a-299248c6d83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rescale(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, landmarks = sample['image'], sample['landmarks']\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        img = transform.resize(image, (new_h, new_w))\n",
    "\n",
    "        # h and w are swapped for landmarks because for images,\n",
    "        # x and y axes are axis 1 and 0 respectively\n",
    "        landmarks = landmarks * [new_w / w, new_h / h]\n",
    "\n",
    "        return {'image': img, 'landmarks': landmarks}\n",
    "\n",
    "\n",
    "class RandomCrop(object):\n",
    "    \"\"\"Crop randomly the image in a sample.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If int, square crop\n",
    "            is made.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, landmarks = sample['image'], sample['landmarks']\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "        new_h, new_w = self.output_size\n",
    "\n",
    "        top = np.random.randint(0, h - new_h + 1)\n",
    "        left = np.random.randint(0, w - new_w + 1)\n",
    "\n",
    "        image = image[top: top + new_h,\n",
    "                      left: left + new_w]\n",
    "\n",
    "        landmarks = landmarks - [left, top]\n",
    "\n",
    "        return {'image': image, 'landmarks': landmarks}\n",
    "        \n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'], sample['label']\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        return {'image': torch.from_numpy(image),\n",
    "                'label': torch.from_numpy(label)}\n",
    "\n",
    "\n",
    "class Normalize(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, image):\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        image = 2*(image/255.0) - 1 # between -1 and +1\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9fb143c-964b-4496-8d4e-f24e3d6ee5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transforms_train = transforms.Compose([Normalize(),\n",
    "                                transforms.Resize(256),\n",
    "                               transforms.RandomCrop(224)\n",
    "                                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b9dcbc3-62e1-45bb-8db3-ded40a8389b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.CustomImageDataset at 0x24361a88eb0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = CustomImageDataset(train_df,transform=transforms_train)\n",
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27f176e0-c34c-4348-89cd-c36515ceb4a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500, 2500)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_df), len(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6d5bc0f-e180-474b-be12-f11e27a5a048",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transforms_test_valid = transforms.Compose([Normalize(),\n",
    "                                transforms.Resize(256),\n",
    "                                transforms.CenterCrop(224)\n",
    "                                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66f58cf4-21ec-4c96-aae9-866ca504eac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = CustomImageDataset(test_df, transform=transforms_test_valid)\n",
    "valid_ds = CustomImageDataset(valid_df, transform=transforms_test_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63f5c117-9cec-4019-a95f-e53f1dbebe77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 2500, 2500)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds), len(test_ds), len(valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8a9f52a-cd81-43f9-9ec4-c4741797895c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': tensor([[[ 0.6087,  0.6985,  0.7399,  ..., -0.4219, -0.4297, -0.4291],\n",
      "         [ 0.6239,  0.7386,  0.8176,  ..., -0.4195, -0.4311, -0.4340],\n",
      "         [ 0.6363,  0.7401,  0.8583,  ..., -0.4195, -0.4311, -0.4353],\n",
      "         ...,\n",
      "         [ 0.7515,  0.7453,  0.7418,  ..., -0.2777, -0.2747, -0.2706],\n",
      "         [ 0.7417,  0.7361,  0.7313,  ..., -0.2735, -0.2699, -0.2674],\n",
      "         [ 0.7519,  0.7473,  0.7442,  ..., -0.2582, -0.2572, -0.2534]],\n",
      "\n",
      "        [[ 0.7476,  0.8166,  0.8444,  ..., -0.4063, -0.4140, -0.4122],\n",
      "         [ 0.7617,  0.8518,  0.9103,  ..., -0.4038, -0.4154, -0.4170],\n",
      "         [ 0.7644,  0.8470,  0.9457,  ..., -0.4038, -0.4154, -0.4183],\n",
      "         ...,\n",
      "         [ 0.7751,  0.7689,  0.7653,  ..., -0.3404, -0.3375, -0.3333],\n",
      "         [ 0.7652,  0.7596,  0.7548,  ..., -0.3362, -0.3326, -0.3301],\n",
      "         [ 0.7755,  0.7709,  0.7677,  ..., -0.3210, -0.3199, -0.3161]],\n",
      "\n",
      "        [[ 0.6973,  0.7622,  0.7794,  ..., -0.4298, -0.4449, -0.4539],\n",
      "         [ 0.7120,  0.7974,  0.8492,  ..., -0.4274, -0.4463, -0.4588],\n",
      "         [ 0.7211,  0.7928,  0.8853,  ..., -0.4274, -0.4463, -0.4601],\n",
      "         ...,\n",
      "         [ 0.8143,  0.8081,  0.8155,  ..., -0.4894, -0.4865, -0.4824],\n",
      "         [ 0.8045,  0.7989,  0.8050,  ..., -0.4853, -0.4817, -0.4791],\n",
      "         [ 0.8147,  0.8101,  0.8180,  ..., -0.4700, -0.4690, -0.4651]]]), 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "for item in train_ds:\n",
    "    print(item)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5fffa53-6710-4e38-842d-1d2f4172f8e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.has_mps or torch.backends.mps.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c32321c3-7bde-4c26-83db-3aa8568b59f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3222977-502e-4d46-9467-be19f67dd581",
   "metadata": {},
   "source": [
    "## Dataloader - Batching shuffling etc\n",
    "#### `https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader`\n",
    "\n",
    "\n",
    "### TODO: optimize dataloader\n",
    "- TFRecords\n",
    "- interleave\n",
    "- .map() operations\n",
    "- batch level `transform`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f40f20a7-569b-4606-9397-22f36965e5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_ds, batch_size=256, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a07730f-c642-4fe1-a870-9c7a88e74e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: How can I increase the batch size?\n",
    "valid_dataloader =  DataLoader(valid_ds, batch_size=1, shuffle=False, num_workers=0)\n",
    "test_dataloader =  DataLoader(test_ds, batch_size=1, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057556e2-4129-4718-9954-9c28e4f4aeed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "208a2ed7-c8af-430e-87ba-bbdb6f3f0c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256, 3, 224, 224]), torch.Size([256]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display image and label.\n",
    "train_iterator = iter(train_dataloader)\n",
    "train_sample = next(train_iterator)\n",
    "train_features, train_labels = train_sample[\"image\"], train_sample[\"label\"]\n",
    "train_features.shape, train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cba03550-9e0e-43d8-88b3-59ea11b216f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#next(train_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e398224-c68b-44a8-877a-8ee823f48f85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3a4fd7f2-46d1-4d68-bc10-ca6f7f0fb371",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3)\n",
    "        self.conv4 = nn.Conv2d(64, 128, 3)\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        \n",
    "        self.fc1 = nn.Linear(128, 32)\n",
    "        self.fc2 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.pool(F.relu(self.conv4(x)))\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dbc0184c-abeb-4d5b-b1bc-56a6803851cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 16, 254, 254]             448\n",
      "         MaxPool2d-2         [-1, 16, 127, 127]               0\n",
      "            Conv2d-3         [-1, 32, 125, 125]           4,640\n",
      "         MaxPool2d-4           [-1, 32, 62, 62]               0\n",
      "            Conv2d-5           [-1, 64, 60, 60]          18,496\n",
      "         MaxPool2d-6           [-1, 64, 30, 30]               0\n",
      "            Conv2d-7          [-1, 128, 28, 28]          73,856\n",
      "         MaxPool2d-8          [-1, 128, 14, 14]               0\n",
      " AdaptiveAvgPool2d-9            [-1, 128, 1, 1]               0\n",
      "           Linear-10                   [-1, 32]           4,128\n",
      "           Linear-11                    [-1, 1]              33\n",
      "================================================================\n",
      "Total params: 101,601\n",
      "Trainable params: 101,601\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 17.75\n",
      "Params size (MB): 0.39\n",
      "Estimated Total Size (MB): 18.89\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "model = SimpleNet().to(device)\n",
    "#model = get_transformer()\n",
    "\n",
    "summary(model, (3, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "99198206-4005-4c16-8fc1-c46af14e9a83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 1])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(train_features.to(device)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17950a7f-700f-4530-b729-2944259bc189",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "544df5be-93ff-4a25-a0c5-dd6c206e4167",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# 1.15.0 or above\n",
    "# pip install numpy==1.23.5\n",
    "# pip install tensorboard==1.15.0\n",
    "# pip install tensorflow==2.7.0\n",
    "\n",
    "writer = SummaryWriter(\"tboard/cats_dogs_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4de6c700-f2f1-4fd0-b638-cae6838ed9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer_to(optim, device):\n",
    "    for param in optim.state.values():\n",
    "        # Not sure there are any global tensors in the state dict\n",
    "        if isinstance(param, torch.Tensor):\n",
    "            param.data = param.data.to(device)\n",
    "            if param._grad is not None:\n",
    "                param._grad.data = param._grad.data.to(device)\n",
    "        elif isinstance(param, dict):\n",
    "            for subparam in param.values():\n",
    "                if isinstance(subparam, torch.Tensor):\n",
    "                    subparam.data = subparam.data.to(device)\n",
    "                    if subparam._grad is not None:\n",
    "                        subparam._grad.data = subparam._grad.data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e74ed0a6-0970-4482-aba2-b3b89c603926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "35d6e36a-533a-4244-8d77-2905c8b24117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-09\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.0001\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=10**-4, eps=1e-9)\n",
    "\n",
    "optimizer_to(optimizer,device)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6e71196a-438f-4a70-b12f-1d352a25510f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BCELoss()"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ignore_index=tokenizer_src.token_to_id('[PAD]'), label_smoothing=0.1\n",
    "\n",
    "# loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "loss_fn = nn.BCELoss().to(device)\n",
    "loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a8d27fbc-837b-474e-8ac3-4eb76b164e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunningMeanMetric:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.sum = 0.0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, value, n=1):\n",
    "        self.sum += value * n\n",
    "        self.count += n\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.sum = 0.0\n",
    "        self.count = 0\n",
    "\n",
    "    def value(self):\n",
    "        return self.sum / self.count if self.count > 0 else float('nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fafae94-4f25-4cc0-9212-01ba6197b8f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ac0967e2-beea-4837-836c-22ec7078984c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SummaryMetric:\n",
    "\n",
    "#     def __init__(self, log_dir, name):\n",
    "#         self._summary_writer = tf.summary.create_file_writer(f\"{log_dir}/{name}\")\n",
    "\n",
    "#     def __call__(self, metrics, epoch, models_dict = None):\n",
    "#         with self._summary_writer.as_default():\n",
    "#             for metric_name, metric_value in metrics.items():\n",
    "#                 if \"image\" in metric_name:\n",
    "#                     if not \"images\" in metric_name:\n",
    "#                         metric_value = [metric_value]\n",
    "#                     tf.summary.image(metric_name, metric_value, max_outputs=16, step=epoch)\n",
    "#                 elif \"histogram\" in metric_name:\n",
    "#                     tf.summary.histogram(metric_name, metric_value, step=epoch)\n",
    "#                 elif \"grad\" in metric_name:\n",
    "#                     tf.summary.histogram(metric_name, metric_value, step=epoch)\n",
    "#                 else: #assume scalar\n",
    "#                     tf.summary.scalar(metric_name, metric_value, step=epoch)\n",
    "\n",
    "#         if models_dict is not None:\n",
    "#             for mname, model in models_dict.items():\n",
    "#                 for mlayer in model.layers():\n",
    "#                     try:\n",
    "#                         tf.summary.histogram(f\"{mname}-{mlayer.name}\", mlayer.weights[0], step= epoch)\n",
    "#                     except (ValueError, IndexError):\n",
    "#                         pass\n",
    "#         self._summary_writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62fec43-0190-4598-893e-2d19dbe9788e",
   "metadata": {},
   "source": [
    "## https://pytorch.org/docs/stable/tensorboard.html\n",
    "### https://github.com/sifubro/pytorch-transformer/blob/main/train.py#L8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3696c5ef-a708-44d1-9877-f288b8315ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable CUDA error checking\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3e2c947f-4514-4e0d-b17a-81742cd19177",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 00: 100%|█████████████████████████████████████████████████████████████| 79/79 [01:10<00:00,  1.12it/s]\n",
      "Processing Epoch 01: 100%|█████████████████████████████████████████████████████████████| 79/79 [01:07<00:00,  1.17it/s]\n"
     ]
    }
   ],
   "source": [
    "initial_epoch = 0\n",
    "last_epoch = 2\n",
    "global_step = 0\n",
    "\n",
    "save_dir = \"C://Users/SiFuBrO/Desktop/SCRIPTS!!!!!/GitHub/pytorch-base/checkpoints/cats_dogs_classifier_v1/\"\n",
    "metric_aggregators = {}\n",
    "metric_aggregators[\"train_loss_weighted\"] = RunningMeanMetric(name = \"train_loss_weighted\")\n",
    "\n",
    "for epoch in range(initial_epoch, last_epoch):\n",
    "    torch.cuda.empty_cache()\n",
    "    model.train()\n",
    "    batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n",
    "    train_loss_epoch = []\n",
    "\n",
    "    # reset metric aggregators (i.e. running means)\n",
    "    for mname in  metric_aggregators.keys():\n",
    "        metric_aggregators[mname].reset_states()\n",
    "\n",
    "    for i, batch in enumerate(batch_iterator):\n",
    "\n",
    "        train_features = batch[\"image\"].to(device)\n",
    "        train_labels = batch[\"label\"].to(device)\n",
    "\n",
    "        # model output\n",
    "        output = model(train_features) # (B, 1)\n",
    "        output = torch.squeeze(output) # (B,)\n",
    "        # convert output from (B,1) -> (B,) and val_label cast from Long to float\n",
    "        if torch.isnan(output).any() or torch.isinf(output).any():\n",
    "            print(\"Ouput Tensor contains NaN or infinite values\")\n",
    "            global_step += 1\n",
    "            continue\n",
    "\n",
    "        train_loss = loss_fn(output, train_labels.type(torch.float))\n",
    "        # try:    \n",
    "        #     train_loss = loss_fn(output, train_labels.type(torch.float))\n",
    "        # except Exception as e:\n",
    "        #     print(f\"ERROR epoch {epoch} - iteration: {i}\")\n",
    "        #     print(e)\n",
    "        #     continue\n",
    "        train_loss_float = train_loss.item()\n",
    "        \n",
    "        # Log the loss\n",
    "        writer.add_scalar('train_loss_step', train_loss_float, global_step)\n",
    "        writer.flush()\n",
    "\n",
    "        metric_aggregators[\"train_loss_weighted\"].update(train_loss_float)\n",
    "        writer.add_scalar('train_loss_weighted', metric_aggregators[\"train_loss_weighted\"].value() , global_step)\n",
    "        writer.flush()\n",
    "        \n",
    "        # epoch loss\n",
    "        train_loss_epoch+= [train_loss_float] #.item()\n",
    "\n",
    "        # Backpropagate the loss\n",
    "        train_loss.backward()\n",
    "\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "        # set_to_none=True also sets the .grad attribute of each parameter to None. This can be helpful for memory efficiency and to avoid unintentional errors if gradients are accessed after they've been zeroed out.\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        global_step += 1\n",
    "        \n",
    "    writer.add_scalar('train_loss_epoch', np.mean(train_loss_epoch), global_step)\n",
    "    writer.flush()\n",
    "    \n",
    "    # Run validation at the end of every epoch\n",
    "    model.eval()\n",
    "    total_val_loss = []\n",
    "    val_outputs = []\n",
    "    y_true_labels = []\n",
    "    valid_iterator = iter(valid_dataloader)\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_iterator:\n",
    "            val_features =  batch[\"image\"].to(device)\n",
    "            # TODO: log some images in tensorboard\n",
    "            \n",
    "            val_labels =  batch[\"label\"].to(device)\n",
    "            val_output = model(val_features)[0] # from (1,1) [[0.67442]]  to-> (1) i.e., val_output = [0.67442] for examples\n",
    "            #val_labels is (1,) size i.e, [0] or [1]   \n",
    "\n",
    "            # validation loss\n",
    "            val_loss = loss_fn(val_output, val_labels.type(torch.float)).item()\n",
    "            total_val_loss += [val_loss]\n",
    "            \n",
    "            val_outputs += [val_output.item()]  # val_output = [0.67442] for examples \n",
    "            y_true_labels += [val_labels.item()]\n",
    "        \n",
    "        # Log the validation loss\n",
    "        writer.add_scalar('valid_loss', np.mean(total_val_loss), global_step)\n",
    "        writer.flush()\n",
    "\n",
    "        # Create ROC plots\n",
    "        fpr, tpr, thresholds = roc_curve(np.array(y_true_labels), np.array(val_outputs), pos_label=1)\n",
    "        f, (ax1, ax2) = plt.subplots(2,1, figsize=(8,12))\n",
    "        ax1.plot(fpr, tpr)\n",
    "        ax1.set_xlabel(\"FPR\")\n",
    "        ax1.set_ylabel(\"TPR\")\n",
    "        ax1.set_xscale('log', base=10)\n",
    "        ax1.set_ylim(0.0, 1.01)\n",
    "        ax1.set_title(\"ROC on validation set\")\n",
    "\n",
    "        ax2.plot(fpr, thresholds)\n",
    "        ax2.set_xlabel(\"FPR\")\n",
    "        ax2.set_ylabel(\"Thresholds\")\n",
    "        ax2.set_ylim(0.0, 1.01)\n",
    "\n",
    "        plt.axvline(x=0.005, color='black', ls=\":\", label=\"FPR=0.5%\")\n",
    "        plt.axvline(x=0.01, color='black', ls=\"--\", label=\"FPR=1%\")\n",
    "        writer.add_figure('ROC plot', plt.gcf(), global_step)\n",
    "        writer.flush()\n",
    "\n",
    "    \n",
    "    # Save the model at the end of every epoch\n",
    "    model_filename = f\"{save_dir}/{epoch}.pt\"\n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'global_step': global_step\n",
    "        }, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f9bf8869-fbb7-4fc2-83fb-05d96081b9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8b49fb-6f84-431d-b420-57c750c9b877",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfac7c10-14fe-426b-b3c0-70534f8b364b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Resume Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beef69c1-de39-4090-a87f-fbeac1e68942",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2634dd71-416d-4216-8a2e-aa29c73c7a89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfca564-5f1c-4de3-8858-83d3946cdc6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5ee783-fa13-440e-a9b3-aa67f4657527",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9a2b88-bb5c-4d11-bda4-14759da5f681",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817c43b3-3747-472b-85da-0ec19a70567d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3f3b8f87-2e1e-471f-bf50-6799d6680ded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6907, device='cuda:0')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(torch.unsqueeze(val_output,dim=0),val_labels.type(torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aad9722-f0f4-4c32-94ad-7e703dc3407b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c0546c-c6ca-497c-adbe-2c4401f3e252",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63b6be3-7085-48db-952b-e01cbd25a676",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc7f707-430b-4275-8ff0-9ac8a811d910",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7e7957-9572-4ce2-9c7e-509770ae75fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "63657022-1b45-4bf7-947f-2b4855315171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(batch[\"image\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7655fbb2-a54d-4952-93ee-b9ccc146816a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,\n",
       "        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
       "        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "33edc293-9c45-458f-a6a1-0dfedb1ca44c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[-0.600116  , -0.7715232 , -0.98809004, ..., -0.51925546,\n",
       "          -0.4499052 , -0.35490334],\n",
       "         [-0.8513372 , -0.61788774, -0.8683329 , ..., -0.52104306,\n",
       "          -0.45699853, -0.36221644],\n",
       "         [-0.9254856 , -0.64325154, -0.6304443 , ..., -0.52457213,\n",
       "          -0.46140182, -0.36893332],\n",
       "         ...,\n",
       "         [-0.76769507, -0.7446293 , -0.7324017 , ..., -0.9124992 ,\n",
       "          -0.9696112 , -0.9795303 ],\n",
       "         [-0.73559904, -0.72904885, -0.7033951 , ..., -0.91429484,\n",
       "          -0.9684049 , -0.98730886],\n",
       "         [-0.7249031 , -0.7122105 , -0.7027049 , ..., -0.90359616,\n",
       "          -0.95838463, -0.98961145]],\n",
       "\n",
       "        [[-0.600116  , -0.7715232 , -0.98809004, ..., -0.5035692 ,\n",
       "          -0.43421888, -0.33921707],\n",
       "         [-0.8513372 , -0.61788774, -0.8683329 , ..., -0.5053568 ,\n",
       "          -0.44131222, -0.34653017],\n",
       "         [-0.9254856 , -0.64325154, -0.6304443 , ..., -0.5088858 ,\n",
       "          -0.4457155 , -0.35324705],\n",
       "         ...,\n",
       "         [-0.81475395, -0.7916882 , -0.77946055, ..., -0.9167268 ,\n",
       "          -0.9774543 , -0.98836005],\n",
       "         [-0.78265786, -0.77610767, -0.750454  , ..., -0.9204043 ,\n",
       "          -0.9762481 , -0.99562514],\n",
       "         [-0.7719619 , -0.75926936, -0.7497637 , ..., -0.9114393 ,\n",
       "          -0.96622777, -0.99745464]],\n",
       "\n",
       "        [[-0.600116  , -0.7715232 , -0.98809004, ..., -0.69180447,\n",
       "          -0.6224543 , -0.5274524 ],\n",
       "         [-0.8513372 , -0.61788774, -0.8683329 , ..., -0.693592  ,\n",
       "          -0.6295476 , -0.5347655 ],\n",
       "         [-0.9254856 , -0.64325154, -0.6304443 , ..., -0.69712114,\n",
       "          -0.6339509 , -0.5414824 ],\n",
       "         ...,\n",
       "         [-0.8050191 , -0.78535616, -0.76463234, ..., -0.9667891 ,\n",
       "          -0.9936107 , -0.99031425],\n",
       "         [-0.7669716 , -0.7604214 , -0.73631334, ..., -0.9626206 ,\n",
       "          -0.9951408 , -0.99849975],\n",
       "         [-0.76045877, -0.7441418 , -0.74466306, ..., -0.9464274 ,\n",
       "          -0.9849909 , -0.9972975 ]]],\n",
       "\n",
       "\n",
       "       [[[ 0.00278081, -0.01832441, -0.02445632, ..., -0.37354723,\n",
       "          -0.3324774 , -0.25276273],\n",
       "         [-0.0013469 , -0.02513146, -0.03242201, ..., -0.31373215,\n",
       "          -0.24637558, -0.17346832],\n",
       "         [-0.00550134, -0.03472706, -0.04544674, ..., -0.24742974,\n",
       "          -0.16299996, -0.10373719],\n",
       "         ...,\n",
       "         [-0.2891466 , -0.27880013, -0.2629556 , ..., -0.04681039,\n",
       "          -0.0993652 , -0.14146818],\n",
       "         [-0.24641486, -0.22543335, -0.22511029, ..., -0.0338759 ,\n",
       "          -0.08001127, -0.10304826],\n",
       "         [-0.26031193, -0.3127863 , -0.31662095, ..., -0.05067626,\n",
       "          -0.06804356, -0.03252602]],\n",
       "\n",
       "        [[-0.31094474, -0.3320499 , -0.34117645, ..., -0.7186453 ,\n",
       "          -0.67757547, -0.5978608 ],\n",
       "         [-0.31507242, -0.33885694, -0.34914213, ..., -0.65883017,\n",
       "          -0.5914737 , -0.51856637],\n",
       "         [-0.31922683, -0.34845254, -0.36216685, ..., -0.5925278 ,\n",
       "          -0.508098  , -0.44883522],\n",
       "         ...,\n",
       "         [-0.4851938 , -0.485606  , -0.47836787, ..., -0.33910984,\n",
       "          -0.37273413, -0.39916193],\n",
       "         [-0.47350484, -0.45264703, -0.45834336, ..., -0.32310942,\n",
       "          -0.35338017, -0.36016706],\n",
       "         [-0.50859624, -0.5610707 , -0.5669173 , ..., -0.33618093,\n",
       "          -0.3342155 , -0.27978972]],\n",
       "\n",
       "        [[-0.5073084 , -0.53368986, -0.5510874 , ..., -0.9460963 ,\n",
       "          -0.90502644, -0.8253118 ],\n",
       "         [-0.51527405, -0.54165554, -0.55905306, ..., -0.88628125,\n",
       "          -0.81892467, -0.74601734],\n",
       "         [-0.52314836, -0.5523741 , -0.5720778 , ..., -0.81997883,\n",
       "          -0.735549  , -0.6762862 ],\n",
       "         ...,\n",
       "         [-0.6159024 , -0.6199009 , -0.61421347, ..., -0.5862578 ,\n",
       "          -0.62071985, -0.64243627],\n",
       "         [-0.614561  , -0.59374446, -0.5994586 , ..., -0.57179034,\n",
       "          -0.6013659 , -0.6037289 ],\n",
       "         [-0.6549811 , -0.7074555 , -0.71330214, ..., -0.574463  ,\n",
       "          -0.5717846 , -0.51293814]]],\n",
       "\n",
       "\n",
       "       [[[-0.30227712, -0.2747685 , -0.2664212 , ...,  0.445238  ,\n",
       "           0.48309153,  0.48215428],\n",
       "         [-0.28374726, -0.26454777, -0.25993642, ...,  0.48502284,\n",
       "           0.5278927 ,  0.52450967],\n",
       "         [-0.29165956, -0.2888806 , -0.29135165, ...,  0.5002321 ,\n",
       "           0.5423399 ,  0.5513178 ],\n",
       "         ...,\n",
       "         [ 0.3101107 ,  0.31862372,  0.32405394, ..., -0.9321449 ,\n",
       "          -0.87938184, -0.833962  ],\n",
       "         [ 0.3136171 ,  0.3218669 ,  0.3280226 , ..., -0.9103874 ,\n",
       "          -0.87174964, -0.82930994],\n",
       "         [ 0.31995603,  0.33735892,  0.3474824 , ..., -0.8519253 ,\n",
       "          -0.83717346, -0.81112516]],\n",
       "\n",
       "        [[-0.14326143, -0.08562449, -0.04958836, ...,  0.5862261 ,\n",
       "           0.62228715,  0.6158055 ],\n",
       "         [-0.13720885, -0.080658  , -0.04310361, ...,  0.63723874,\n",
       "           0.67897785,  0.6699506 ],\n",
       "         [-0.15290041, -0.1064342 , -0.07596223, ...,  0.6681647 ,\n",
       "           0.70936537,  0.70488757],\n",
       "         ...,\n",
       "         [ 0.5008691 ,  0.48980582,  0.4760312 , ..., -0.89660513,\n",
       "          -0.8426301 , -0.8027806 ],\n",
       "         [ 0.51173675,  0.4986413 ,  0.48112485, ..., -0.9127587 ,\n",
       "          -0.8694629 , -0.83026755],\n",
       "         [ 0.5234881 ,  0.51658523,  0.5006646 , ..., -0.8802506 ,\n",
       "          -0.861432  , -0.8363694 ]],\n",
       "\n",
       "        [[ 0.0427973 ,  0.10448824,  0.1400345 , ...,  0.6493225 ,\n",
       "           0.68627983,  0.6821991 ],\n",
       "         [ 0.0510265 ,  0.11208183,  0.14651924, ...,  0.71015036,\n",
       "           0.7524548 ,  0.7456992 ],\n",
       "         [ 0.0375345 ,  0.08527675,  0.11318891, ...,  0.75061923,\n",
       "           0.79206926,  0.7904906 ],\n",
       "         ...,\n",
       "         [ 0.65263444,  0.62620187,  0.59437007, ..., -0.8957957 ,\n",
       "          -0.86257845, -0.8224034 ],\n",
       "         [ 0.6233982 ,  0.61170065,  0.6020571 , ..., -0.91785145,\n",
       "          -0.8974942 , -0.84415   ],\n",
       "         [ 0.6037289 ,  0.6158517 ,  0.62983537, ..., -0.87839955,\n",
       "          -0.8821198 , -0.84076303]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[ 0.478044  ,  0.53470933,  0.543645  , ..., -0.2707277 ,\n",
       "          -0.32715362, -0.33349192],\n",
       "         [ 0.4752625 ,  0.52588934,  0.5341021 , ..., -0.2931223 ,\n",
       "          -0.35143852, -0.36315146],\n",
       "         [ 0.46251008,  0.5068767 ,  0.52077305, ..., -0.31180063,\n",
       "          -0.36554638, -0.38454628],\n",
       "         ...,\n",
       "         [-0.19428918, -0.12556404, -0.0466258 , ..., -0.26444525,\n",
       "          -0.29465076, -0.28546858],\n",
       "         [ 0.00982326, -0.1251396 , -0.16918546, ..., -0.2769733 ,\n",
       "          -0.31284463, -0.27361447],\n",
       "         [ 0.6787856 ,  0.64725673,  0.58384615, ..., -0.24410686,\n",
       "          -0.27272153, -0.22049361]],\n",
       "\n",
       "        [[ 0.28196558,  0.32640755,  0.33188036, ..., -0.7099761 ,\n",
       "          -0.7370103 , -0.73985827],\n",
       "         [ 0.27918407,  0.31758755,  0.3223373 , ..., -0.7314729 ,\n",
       "          -0.7480944 , -0.75054187],\n",
       "         [ 0.26643166,  0.29857492,  0.30900836, ..., -0.7484897 ,\n",
       "          -0.7485055 , -0.74914235],\n",
       "         ...,\n",
       "         [-0.3377191 , -0.27055615, -0.18939418, ..., -0.6809345 ,\n",
       "          -0.68090105, -0.6717179 ],\n",
       "         [-0.11404303, -0.2505563 , -0.29239523, ..., -0.6984234 ,\n",
       "          -0.6892182 , -0.63647866],\n",
       "         [ 0.5780637 ,  0.54496896,  0.48378715, ..., -0.67424047,\n",
       "          -0.6490254 , -0.572419  ]],\n",
       "\n",
       "        [[ 0.31333813,  0.36389178,  0.37109604, ..., -0.6409433 ,\n",
       "          -0.6925545 , -0.6974594 ],\n",
       "         [ 0.31055662,  0.3550718 ,  0.36155298, ..., -0.65719074,\n",
       "          -0.704438  , -0.71745294],\n",
       "         [ 0.29780424,  0.33605915,  0.34822404, ..., -0.67467606,\n",
       "          -0.70906025, -0.7243424 ],\n",
       "         ...,\n",
       "         [-0.37821126, -0.30923742, -0.2306532 , ..., -0.7145866 ,\n",
       "          -0.70540106, -0.6829182 ],\n",
       "         [-0.1405997 , -0.27581674, -0.3195007 , ..., -0.74063385,\n",
       "          -0.7208296 , -0.6589074 ],\n",
       "         [ 0.5624258 ,  0.5297518 ,  0.4679712 , ..., -0.71878654,\n",
       "          -0.69140726, -0.6144497 ]]],\n",
       "\n",
       "\n",
       "       [[[-0.405247  , -0.4195341 , -0.47473913, ...,  0.40392157,\n",
       "           0.4039216 ,  0.4133997 ],\n",
       "         [-0.406529  , -0.41274303, -0.46430522, ...,  0.40647537,\n",
       "           0.40647542,  0.4149248 ],\n",
       "         [-0.39190352, -0.39303637, -0.44014418, ...,  0.41592312,\n",
       "           0.41592315,  0.4205668 ],\n",
       "         ...,\n",
       "         [-0.9196396 , -0.93336105, -0.9454831 , ..., -0.14171328,\n",
       "          -0.2696422 , -0.370381  ],\n",
       "         [-0.8815025 , -0.8948146 , -0.8862839 , ..., -0.18547426,\n",
       "          -0.28217772, -0.36967734],\n",
       "         [-0.91915834, -0.9118678 , -0.89316374, ..., -0.20523943,\n",
       "          -0.28195927, -0.3650099 ]],\n",
       "\n",
       "        [[-0.44215742, -0.45691302, -0.5115163 , ...,  0.2313726 ,\n",
       "           0.23137262,  0.24085067],\n",
       "         [-0.45736694, -0.46705586, -0.5141536 , ...,  0.23392639,\n",
       "           0.23392642,  0.24237576],\n",
       "         [-0.4577539 , -0.46250844, -0.50496316, ...,  0.24337411,\n",
       "           0.24337414,  0.24801776],\n",
       "         ...,\n",
       "         [-0.9411483 , -0.9202323 , -0.900558  , ..., -0.21346468,\n",
       "          -0.30870503, -0.38290793],\n",
       "         [-0.9051534 , -0.88579917, -0.8433026 , ..., -0.27016848,\n",
       "          -0.32299912, -0.38220423],\n",
       "         [-0.8738375 , -0.86745226, -0.8418314 , ..., -0.2808737 ,\n",
       "          -0.32021824, -0.36811763]],\n",
       "\n",
       "        [[-0.49080798, -0.5055636 , -0.56016684, ..., -0.06200696,\n",
       "          -0.06200697, -0.05252887],\n",
       "         [-0.5033204 , -0.51241964, -0.56027496, ..., -0.05626974,\n",
       "          -0.05626974, -0.04782036],\n",
       "         [-0.48894262, -0.49188626, -0.5366676 , ..., -0.05199543,\n",
       "          -0.04954697, -0.04714497],\n",
       "         ...,\n",
       "         [-0.9518555 , -0.955702  , -0.95055294, ..., -0.25941777,\n",
       "          -0.2648136 , -0.28560832],\n",
       "         [-0.91008866, -0.92017317, -0.8925209 , ..., -0.30907607,\n",
       "          -0.27822843, -0.28305572],\n",
       "         [-0.896981  , -0.9132092 , -0.8948883 , ..., -0.30906692,\n",
       "          -0.27817294, -0.28591338]]],\n",
       "\n",
       "\n",
       "       [[[ 0.11953695,  0.10668058,  0.0843491 , ..., -0.03642376,\n",
       "          -0.07441903, -0.13576227],\n",
       "         [ 0.12610346,  0.10528553,  0.07505769, ..., -0.02872272,\n",
       "          -0.04635369, -0.13756752],\n",
       "         [ 0.10342054,  0.07380222,  0.04163491, ..., -0.06257917,\n",
       "          -0.09514735, -0.18258774],\n",
       "         ...,\n",
       "         [ 0.3729438 ,  0.4437793 ,  0.54921377, ...,  0.52053046,\n",
       "           0.5106165 ,  0.5227998 ],\n",
       "         [ 0.62752056,  0.6576028 ,  0.65575516, ...,  0.50291675,\n",
       "           0.47633123,  0.4860087 ],\n",
       "         [ 0.6235327 ,  0.65836424,  0.5941582 , ...,  0.5435474 ,\n",
       "           0.52166826,  0.50832236]],\n",
       "\n",
       "        [[ 0.10385066,  0.0909943 ,  0.06866283, ..., -0.05021021,\n",
       "          -0.08869302, -0.14780416],\n",
       "         [ 0.11041719,  0.08959924,  0.05937142, ..., -0.044409  ,\n",
       "          -0.06203996, -0.1532538 ],\n",
       "         [ 0.08773427,  0.05811594,  0.02594864, ..., -0.07826544,\n",
       "          -0.11083362, -0.19827402],\n",
       "         ...,\n",
       "         [ 0.46532398,  0.5312795 ,  0.61323994, ...,  0.59645325,\n",
       "           0.58867764,  0.60123116],\n",
       "         [ 0.7514727 ,  0.777066  ,  0.7568235 , ...,  0.5731646 ,\n",
       "           0.54940474,  0.5644401 ],\n",
       "         [ 0.7612013 ,  0.791952  ,  0.71649027, ...,  0.6103969 ,\n",
       "           0.59294194,  0.5845068 ]],\n",
       "\n",
       "        [[ 0.11185893,  0.09900256,  0.07667108, ..., -0.03966907,\n",
       "          -0.07668927, -0.14249676],\n",
       "         [ 0.13101876,  0.11020081,  0.079973  , ..., -0.02087959,\n",
       "          -0.03851055, -0.12972438],\n",
       "         [ 0.11896381,  0.08934548,  0.05717818, ..., -0.05473603,\n",
       "          -0.08730421, -0.1747446 ],\n",
       "         ...,\n",
       "         [ 0.5193401 ,  0.58533883,  0.67571837, ...,  0.6953056 ,\n",
       "           0.67545205,  0.677497  ],\n",
       "         [ 0.8582374 ,  0.88451374,  0.8693738 , ...,  0.69932115,\n",
       "           0.6599073 ,  0.6561897 ],\n",
       "         [ 0.9200688 ,  0.9528599 ,  0.883026  , ...,  0.75119334,\n",
       "           0.7168569 ,  0.6853653 ]]]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"image\"].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e155ed-59aa-4c0c-89f1-85882e20c9b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bacada-e3ef-49d4-b564-34e055beff4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c478019b-dc56-4045-b720-afef4dca2d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display image and label.\n",
    "valid_iterator = iter(valid_dataloader)\n",
    "valid_sample = next(valid_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "19c9926f-66aa-4b63-8347-b0023da86dc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 224, 224]), torch.Size([1]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_features, val_labels = valid_sample[\"image\"],  valid_sample[\"label\"]\n",
    "\n",
    "val_features.shape, val_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "137fe289-85f2-4411-ba3b-79fd58dbe0bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_output = model(val_features)\n",
    "val_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d6bac3c7-f2a4-4954-925f-f7d233716656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1322]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "64c2e50a-a00b-47e5-8ac7-c1a5787d5b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1322], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_output.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ca282a54-a4e3-4343-8112-cd1886263de6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "246676f4-313c-40e1-9f5c-f65891b2be05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_labels.type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "01d39811-20f8-4345-b1a8-45bf5e7484df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1418, grad_fn=<BinaryCrossEntropyBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss = loss_fn(val_output.view(-1), val_labels.type(torch.float))\n",
    "val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e672a35b-0377-4547-a252-9a4cc3a7d7b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ce2ddb18-6681-4fdb-8d57-a34d2fcf8419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0599, -0.1396]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3e0523-8b31-40fc-a6b5-e377897b5604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a787fb-1831-4b22-abde-6de0bb7f253f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb8346d-59f3-4f16-b4f5-60cc8e17914a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84916dfc-e4fb-428b-8235-b67a039e7c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b73978-73ad-4b8a-9ae6-6b5405d68d51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1692a7b4-797d-4deb-9f59-1e9b6934e522",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a34695cf-690d-4b1c-8914-7e952b600040",
   "metadata": {},
   "source": [
    "## TODO: Tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cd2c5d-b74a-41be-8b7f-1688a5595f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config():\n",
    "    return {\n",
    "        \"batch_size\": 8,\n",
    "        \"num_epochs\": 20,\n",
    "        \"lr\": 10**-4,\n",
    "        \"seq_len\": 350,\n",
    "        \"d_model\": 512,\n",
    "        \"datasource\": 'opus_books',\n",
    "        \"lang_src\": \"en\",\n",
    "        \"lang_tgt\": \"it\",\n",
    "        \"model_folder\": \"weights\",\n",
    "        \"model_basename\": \"tmodel_\",\n",
    "        \"preload\": \"latest\",\n",
    "        \"tokenizer_file\": \"tokenizer_{0}.json\",\n",
    "        \"experiment_name\": \"runs/tmodel\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2e692e31-6163-4d77-8101-b229a118acea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3]), torch.Size([3, 5]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "\n",
    "target.shape, input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00af4a85-a654-4a56-835c-4697989b7f0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a48de5e-e1a4-4864-8e4f-48cdee3e9467",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODOS:\n",
    "- Multi gpu training\n",
    "- Load model and resume training\n",
    "- Freeze part of the model and train the rest\n",
    "- Initialize custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99598e5a-123b-4b0c-8cb2-be8b41ffba95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7785271b-8d89-4322-9da9-aa5d88d985b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1c3c42-4c09-4333-8a97-1e00639276cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba84e235-ca43-4fd9-b915-d170e6f6e2eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196ca67e-8773-42c0-99ef-72166cac5ffb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6653483-35e2-4e35-9a2a-0836222fb8de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a4a877-bf94-4829-8469-0f9f879de943",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbffb0fa-2452-4cab-ad88-756b81947f1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce2b6d5-f4ad-4bc6-8779-ea803bc067d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e01c06-734c-46d7-815a-d9c3b63685f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9567032e-09b3-4b58-a448-f152acd4edb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
