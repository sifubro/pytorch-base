{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e4515c2-8ef8-4aa8-89af-83ebdf9bab96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "import glob\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8c76526-9db0-41ec-ad49-a0f041d1d8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e81e1a-5378-4464-bbb6-54c1790cc586",
   "metadata": {},
   "source": [
    "### csv with filenames and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1912d570-23d4-4221-b0cb-192f15a4d742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = glob.glob(\"D://DATASETS/DogsVsCats/train-val-test/*.jpg\")\n",
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ea7e89e-2212-4339-a102-a2132e236004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D://DATASETS/DogsVsCats/train-val-test\\cat.0.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D://DATASETS/DogsVsCats/train-val-test\\cat.1.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D://DATASETS/DogsVsCats/train-val-test\\cat.10.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filename  label\n",
       "0   D://DATASETS/DogsVsCats/train-val-test\\cat.0.jpg      0\n",
       "1   D://DATASETS/DogsVsCats/train-val-test\\cat.1.jpg      0\n",
       "2  D://DATASETS/DogsVsCats/train-val-test\\cat.10.jpg      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [1 if \"dog\" in fname else 0 for fname in images]\n",
    "full_df = pd.DataFrame()\n",
    "full_df[\"filename\"] = images\n",
    "full_df[\"label\"] = labels\n",
    "full_df[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b7bade-1933-4f32-8602-3370545c95a1",
   "metadata": {},
   "source": [
    "### Shuffle dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9a4166a-ebd4-493b-8ac7-06db0286d11b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5680</td>\n",
       "      <td>D://DATASETS/DogsVsCats/train-val-test\\cat.386...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21452</td>\n",
       "      <td>D://DATASETS/DogsVsCats/train-val-test\\dog.680...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21516</td>\n",
       "      <td>D://DATASETS/DogsVsCats/train-val-test\\dog.686...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>96</td>\n",
       "      <td>D://DATASETS/DogsVsCats/train-val-test\\cat.100...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4960</td>\n",
       "      <td>D://DATASETS/DogsVsCats/train-val-test\\cat.321...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                           filename  label\n",
       "0   5680  D://DATASETS/DogsVsCats/train-val-test\\cat.386...      0\n",
       "1  21452  D://DATASETS/DogsVsCats/train-val-test\\dog.680...      1\n",
       "2  21516  D://DATASETS/DogsVsCats/train-val-test\\dog.686...      1\n",
       "3     96  D://DATASETS/DogsVsCats/train-val-test\\cat.100...      0\n",
       "4   4960  D://DATASETS/DogsVsCats/train-val-test\\cat.321...      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df = full_df.sample(frac=1).reset_index()\n",
    "full_df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8a59721-7309-4112-ae2d-1025d1fcbafd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12500"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.label.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09a0293d-519c-43b6-8e24-62a607a47c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SiFuBrO\\anaconda3\\envs\\torch\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "dogs_df = full_df[full_df.label==1].sample(frac=1).reset_index(drop=True)\n",
    "cats_df = full_df[full_df.label==0].sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "dogs = {\"train\":None, \"test\": None, \"valid\":None}\n",
    "cats = {\"train\":None, \"test\": None, \"valid\":None}\n",
    "\n",
    "dogs[\"train\"], dogs[\"test\"], dogs[\"valid\"] = np.split(dogs_df, [int(0.8*len(dogs_df)), int(0.9*len(dogs_df))])\n",
    "cats[\"train\"], cats[\"test\"], cats[\"valid\"] = np.split(cats_df, [int(0.8*len(cats_df)), int(0.9*len(cats_df))])\n",
    "\n",
    "train_df = pd.concat([dogs[\"train\"], cats[\"train\"]]).sample(frac=1).reset_index(drop=True)\n",
    "test_df = pd.concat([dogs[\"test\"], cats[\"test\"]]).sample(frac=1).reset_index(drop=True)\n",
    "valid_df = pd.concat([dogs[\"valid\"], cats[\"valid\"]]).sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3adc6af5-0d2f-477c-8e48-9ac06c6ae15d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 2500, 2500)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df), len(test_df), len(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad8d69e8-79d7-4e4a-aee9-39ff1ca45924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4472</td>\n",
       "      <td>D://DATASETS/DogsVsCats/train-val-test\\cat.277...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18230</td>\n",
       "      <td>D://DATASETS/DogsVsCats/train-val-test\\dog.390...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7982</td>\n",
       "      <td>D://DATASETS/DogsVsCats/train-val-test\\cat.593...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                           filename  label\n",
       "0   4472  D://DATASETS/DogsVsCats/train-val-test\\cat.277...      0\n",
       "1  18230  D://DATASETS/DogsVsCats/train-val-test\\dog.390...      1\n",
       "2   7982  D://DATASETS/DogsVsCats/train-val-test\\cat.593...      0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "308029a0-468d-4602-a07d-82172031c0de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.label.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc93def-fd91-4ec1-b6c6-9bc5bb80c40b",
   "metadata": {},
   "source": [
    "### `Dataset` stores the samples and their corresponding labels, and `DataLoader` wraps an iterable around the Dataset to enable easy access to the samples.\n",
    "\n",
    "#### https://pytorch.org/docs/stable/data.html#data-loading-order-and-sampler\n",
    "#### https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset\n",
    "\n",
    "### TODO: Sampler\n",
    "\n",
    "#### https://pytorch.org/docs/stable/data.html#torch.utils.data.Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f025fa26-46b7-47ba-b478-9a7a4e7f001a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    '''\n",
    "    https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "    '''\n",
    "    def __init__(self, train_df, transform=None, target_transform=None):\n",
    "        self.train_df = train_df\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = train_df.filename[idx]\n",
    "        # read JPEG or PNG image from filepath  --> output (Tensor[image_channels, image_height, image_width])\n",
    "        image =  read_image(img_path)\n",
    "        label =  train_df.label[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            # These are transformation single level\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "            \n",
    "        return {'image': image, 'label': label}  #image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdeb0846-3835-43be-a793-a15bdfa1e5fa",
   "metadata": {},
   "source": [
    "### https://pytorch.org/tutorials/beginner/data_loading_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5ae26bb-9dc8-4b5a-972a-299248c6d83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rescale(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, landmarks = sample['image'], sample['landmarks']\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        img = transform.resize(image, (new_h, new_w))\n",
    "\n",
    "        # h and w are swapped for landmarks because for images,\n",
    "        # x and y axes are axis 1 and 0 respectively\n",
    "        landmarks = landmarks * [new_w / w, new_h / h]\n",
    "\n",
    "        return {'image': img, 'landmarks': landmarks}\n",
    "\n",
    "\n",
    "class RandomCrop(object):\n",
    "    \"\"\"Crop randomly the image in a sample.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If int, square crop\n",
    "            is made.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, landmarks = sample['image'], sample['landmarks']\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "        new_h, new_w = self.output_size\n",
    "\n",
    "        top = np.random.randint(0, h - new_h + 1)\n",
    "        left = np.random.randint(0, w - new_w + 1)\n",
    "\n",
    "        image = image[top: top + new_h,\n",
    "                      left: left + new_w]\n",
    "\n",
    "        landmarks = landmarks - [left, top]\n",
    "\n",
    "        return {'image': image, 'landmarks': landmarks}\n",
    "        \n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'], sample['label']\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        return {'image': torch.from_numpy(image),\n",
    "                'label': torch.from_numpy(label)}\n",
    "\n",
    "\n",
    "class Normalize(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, image):\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        image = 2*(image/255.0) - 1 # between -1 and +1\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9fb143c-964b-4496-8d4e-f24e3d6ee5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transforms_train = transforms.Compose([Normalize(),\n",
    "                                transforms.Resize(256),\n",
    "                               transforms.RandomCrop(224)\n",
    "                                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b9dcbc3-62e1-45bb-8db3-ded40a8389b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.CustomImageDataset at 0x2c718edb450>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = CustomImageDataset(train_df,transform=transforms_train)\n",
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27f176e0-c34c-4348-89cd-c36515ceb4a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500, 2500)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_df), len(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6d5bc0f-e180-474b-be12-f11e27a5a048",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transforms_test_valid = transforms.Compose([Normalize(),\n",
    "                                transforms.Resize(256),\n",
    "                                transforms.CenterCrop(224)\n",
    "                                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66f58cf4-21ec-4c96-aae9-866ca504eac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = CustomImageDataset(test_df, transform=transforms_test_valid)\n",
    "valid_ds = CustomImageDataset(valid_df, transform=transforms_test_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0066ba7f-48b9-4876-9895-e2a0a163e55e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8737e2ca-b002-45d9-835b-c2f77df09b3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63f5c117-9cec-4019-a95f-e53f1dbebe77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 2500, 2500)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds), len(test_ds), len(valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8a9f52a-cd81-43f9-9ec4-c4741797895c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': tensor([[[ 0.3318,  0.3154,  0.3103,  ...,  0.0259, -0.0230, -0.0521],\n",
      "         [ 0.3181,  0.3025,  0.2973,  ...,  0.0520, -0.0079, -0.0456],\n",
      "         [ 0.3234,  0.3149,  0.3097,  ...,  0.0824,  0.0368, -0.0011],\n",
      "         ...,\n",
      "         [ 0.3794,  0.3244,  0.3042,  ...,  0.2973,  0.3225,  0.3490],\n",
      "         [ 0.3462,  0.3201,  0.2999,  ...,  0.3455,  0.3358,  0.3265],\n",
      "         [ 0.3533,  0.3432,  0.3175,  ...,  0.3775,  0.3743,  0.3701]],\n",
      "\n",
      "        [[ 0.2513,  0.2580,  0.2632,  ...,  0.0024, -0.0267, -0.0510],\n",
      "         [ 0.2376,  0.2440,  0.2471,  ...,  0.0285, -0.0195, -0.0463],\n",
      "         [ 0.2430,  0.2548,  0.2548,  ...,  0.0578,  0.0135, -0.0135],\n",
      "         ...,\n",
      "         [ 0.1762,  0.1415,  0.1316,  ...,  0.0777,  0.1029,  0.1302],\n",
      "         [ 0.1472,  0.1415,  0.1298,  ...,  0.1259,  0.1162,  0.1077],\n",
      "         [ 0.1552,  0.1681,  0.1515,  ...,  0.1579,  0.1547,  0.1513]],\n",
      "\n",
      "        [[-0.3901, -0.3956, -0.3956,  ..., -0.5388, -0.5877, -0.6163],\n",
      "         [-0.3941, -0.4023, -0.4023,  ..., -0.5190, -0.5788, -0.6108],\n",
      "         [-0.3864, -0.3805, -0.3805,  ..., -0.5010, -0.5435, -0.5741],\n",
      "         ...,\n",
      "         [-0.3924, -0.4371, -0.4550,  ..., -0.4948, -0.4762, -0.4526],\n",
      "         [-0.4385, -0.4542, -0.4721,  ..., -0.4467, -0.4629, -0.4751],\n",
      "         [-0.4449, -0.4459, -0.4700,  ..., -0.4147, -0.4244, -0.4315]]]), 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "for item in train_ds:\n",
    "    print(item)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5fffa53-6710-4e38-842d-1d2f4172f8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SiFuBrO\\AppData\\Local\\Temp\\ipykernel_34048\\3897690485.py:2: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.has_mps or torch.backends.mps.is_available() else \"cpu\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.has_mps or torch.backends.mps.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c32321c3-7bde-4c26-83db-3aa8568b59f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3222977-502e-4d46-9467-be19f67dd581",
   "metadata": {},
   "source": [
    "## Dataloader - Batching shuffling etc\n",
    "#### `https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader`\n",
    "\n",
    "\n",
    "### TODO: optimize dataloader\n",
    "- TFRecords\n",
    "- interleave\n",
    "- .map() operations\n",
    "- batch level `transform`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f40f20a7-569b-4606-9397-22f36965e5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a07730f-c642-4fe1-a870-9c7a88e74e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: How can I increase the batch size?\n",
    "valid_dataloader =  DataLoader(valid_ds, batch_size=1, shuffle=False, num_workers=0)\n",
    "test_dataloader =  DataLoader(test_ds, batch_size=1, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057556e2-4129-4718-9954-9c28e4f4aeed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "208a2ed7-c8af-430e-87ba-bbdb6f3f0c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 3, 224, 224]), torch.Size([64]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display image and label.\n",
    "train_iterator = iter(train_dataloader)\n",
    "train_sample = next(train_iterator)\n",
    "train_features, train_labels = train_sample[\"image\"], train_sample[\"label\"]\n",
    "train_features.shape, train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cba03550-9e0e-43d8-88b3-59ea11b216f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#next(train_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e398224-c68b-44a8-877a-8ee823f48f85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3a4fd7f2-46d1-4d68-bc10-ca6f7f0fb371",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3)\n",
    "        self.conv4 = nn.Conv2d(64, 128, 3)\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        \n",
    "        self.fc1 = nn.Linear(128, 32)\n",
    "        self.fc2 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.pool(F.relu(self.conv4(x)))\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dbc0184c-abeb-4d5b-b1bc-56a6803851cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 16, 254, 254]             448\n",
      "         MaxPool2d-2         [-1, 16, 127, 127]               0\n",
      "            Conv2d-3         [-1, 32, 125, 125]           4,640\n",
      "         MaxPool2d-4           [-1, 32, 62, 62]               0\n",
      "            Conv2d-5           [-1, 64, 60, 60]          18,496\n",
      "         MaxPool2d-6           [-1, 64, 30, 30]               0\n",
      "            Conv2d-7          [-1, 128, 28, 28]          73,856\n",
      "         MaxPool2d-8          [-1, 128, 14, 14]               0\n",
      " AdaptiveAvgPool2d-9            [-1, 128, 1, 1]               0\n",
      "           Linear-10                   [-1, 32]           4,128\n",
      "           Linear-11                    [-1, 1]              33\n",
      "================================================================\n",
      "Total params: 101,601\n",
      "Trainable params: 101,601\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 17.75\n",
      "Params size (MB): 0.39\n",
      "Estimated Total Size (MB): 18.89\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "model = SimpleNet()\n",
    "#model = get_transformer()\n",
    "\n",
    "summary(model, (3, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "544df5be-93ff-4a25-a0c5-dd6c206e4167",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(\"tboard/cats_dogs_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "35d6e36a-533a-4244-8d77-2905c8b24117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-09\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.0001\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=10**-4, eps=1e-9)\n",
    "\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6e71196a-438f-4a70-b12f-1d352a25510f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BCELoss()"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ignore_index=tokenizer_src.token_to_id('[PAD]'), label_smoothing=0.1\n",
    "\n",
    "# loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "loss_fn = nn.BCELoss().to(device)\n",
    "loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a8d27fbc-837b-474e-8ac3-4eb76b164e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunningMeanMetric:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.sum = 0.0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, value, n=1):\n",
    "        self.sum += value * n\n",
    "        self.count += n\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.sum = 0.0\n",
    "        self.count = 0\n",
    "\n",
    "    def value(self):\n",
    "        return self.sum / self.count if self.count > 0 else float('nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fafae94-4f25-4cc0-9212-01ba6197b8f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0967e2-beea-4837-836c-22ec7078984c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummaryMetric:\n",
    "\n",
    "    def __init__(self, log_dir, name):\n",
    "        self._summary_writer = tf.summary.create_file_writer(f\"{log_dir}/{name}\")\n",
    "\n",
    "    def __call__(self, metrics, epoch, models_dict = None):\n",
    "        with self._summary_writer.as_default():\n",
    "            for metric_name, metric_value in metrics.items():\n",
    "                if \"image\" in metric_name:\n",
    "                    if not \"images\" in metric_name:\n",
    "                        metric_value = [metric_value]\n",
    "                    tf.summary.image(metric_name, metric_value, max_outputs=16, step=epoch)\n",
    "                elif \"histogram\" in metric_name:\n",
    "                    tf.summary.histogram(metric_name, metric_value, step=epoch)\n",
    "                elif \"grad\" in metric_name:\n",
    "                    tf.summary.histogram(metric_name, metric_value, step=epoch)\n",
    "                else: #assume scalar\n",
    "                    tf.summary.scalar(metric_name, metric_value, step=epoch)\n",
    "\n",
    "        if models_dict is not None:\n",
    "            for mname, model in models_dict.items():\n",
    "                for mlayer in model.layers():\n",
    "                    try:\n",
    "                        tf.summary.histogram(f\"{mname}-{mlayer.name}\", mlayer.weights[0], step= epoch)\n",
    "                    except (ValueError, IndexError):\n",
    "                        pass\n",
    "        self._summary_writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62fec43-0190-4598-893e-2d19dbe9788e",
   "metadata": {},
   "source": [
    "## https://pytorch.org/docs/stable/tensorboard.html\n",
    "### https://github.com/sifubro/pytorch-transformer/blob/main/train.py#L8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3e2c947f-4514-4e0d-b17a-81742cd19177",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 00: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 313/313 [11:12<00:00,  2.15s/it]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'CustomImageDataset' object has no attribute 'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 75\u001b[0m\n\u001b[0;32m     72\u001b[0m writer\u001b[38;5;241m.\u001b[39mflush()\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Create ROC plots?\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m fpr, tpr, thresholds \u001b[38;5;241m=\u001b[39m roc_curve(np\u001b[38;5;241m.\u001b[39marray(\u001b[43mvalid_ds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel\u001b[49m), np\u001b[38;5;241m.\u001b[39marray(val_outputs), pos_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     76\u001b[0m f, (ax1, ax2) \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m12\u001b[39m))\n\u001b[0;32m     77\u001b[0m ax1\u001b[38;5;241m.\u001b[39mplot(fpr, tpr)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CustomImageDataset' object has no attribute 'label'"
     ]
    }
   ],
   "source": [
    "initial_epoch = 0\n",
    "last_epoch = 2\n",
    "global_step = 0\n",
    "\n",
    "save_dir = \"./checkpoints/cats_dogs_classifier_v1/\"\n",
    "metric_aggregators = {}\n",
    "metric_aggregators[\"train_loss_weighted\"] = RunningMeanMetric(name = \"train_loss_weighted\")\n",
    "\n",
    "for epoch in range(initial_epoch, last_epoch):\n",
    "    torch.cuda.empty_cache()\n",
    "    model.train()\n",
    "    batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n",
    "    train_loss_epoch = []\n",
    "\n",
    "    # reset metric aggregators (i.e. running means)\n",
    "    for mname in  metric_aggregators.keys():\n",
    "        metric_aggregators[mname].reset_states()\n",
    "\n",
    "    for batch in batch_iterator:\n",
    "\n",
    "        train_features = batch[\"image\"] #.to_device()\n",
    "        train_labels = batch[\"label\"] #.to_device()\n",
    "\n",
    "        # model output\n",
    "        output = model(train_features) # (B, 1)\n",
    "        # convert output from (B,1) -> (B,) and val_label cast from Long to float\n",
    "        train_loss = loss_fn(output.view(-1), train_labels.type(torch.float))\n",
    "        \n",
    "        # Log the loss\n",
    "        writer.add_scalar('train_loss_step', train_loss.item(), global_step)\n",
    "        writer.flush()\n",
    "\n",
    "        metric_aggregators[\"train_loss_weighted\"].update(train_loss.item())\n",
    "        writer.add_scalar('train_loss_weighted', metric_aggregators[\"train_loss_weighted\"].value() , global_step)\n",
    "        writer.flush()\n",
    "        \n",
    "        # epoch loss\n",
    "        train_loss_epoch+= [train_loss.item()] #.item()\n",
    "\n",
    "        # Backpropagate the loss\n",
    "        train_loss.backward()\n",
    "\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "        # set_to_none=True also sets the .grad attribute of each parameter to None. This can be helpful for memory efficiency and to avoid unintentional errors if gradients are accessed after they've been zeroed out.\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        global_step += 1\n",
    "        \n",
    "    writer.add_scalar('train_loss_epoch', np.mean(train_loss_epoch), global_step)\n",
    "    writer.flush()\n",
    "    \n",
    "    # Run validation at the end of every epoch\n",
    "    model.eval()\n",
    "    total_val_loss = []\n",
    "    val_outputs = []\n",
    "    valid_iterator = iter(valid_dataloader)\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_iterator:\n",
    "            val_features =  batch[\"image\"] #.to_device()\n",
    "            val_labels =  batch[\"label\"] #.to_device()\n",
    "            val_output = model(val_features) # (B,1)\n",
    "\n",
    "            # validation loss\n",
    "            val_loss = loss_fn(val_output.view(-1), val_labels.type(torch.float))\n",
    "            total_val_loss += [val_loss.item()]\n",
    "            \n",
    "            val_outputs += val_output.view(-1)  # from (B,1) -> (B,)\n",
    "        \n",
    "        # Log the validation loss\n",
    "        writer.add_scalar('valid_loss', np.mean(total_val_loss), global_step)\n",
    "        writer.flush()\n",
    "\n",
    "        # Create ROC plots?\n",
    "        fpr, tpr, thresholds = roc_curve(np.array(valid_ds.label), np.array(val_outputs), pos_label=1)\n",
    "        f, (ax1, ax2) = plt.subplots(2,1, figsize=(8,12))\n",
    "        ax1.plot(fpr, tpr)\n",
    "        ax1.set_xlabel(\"FPR\")\n",
    "        ax1.set_ylabel(\"TPR\")\n",
    "        ax1.set_xscale('log', base=10)\n",
    "        ax1.set_ylim(0.0, 1.01)\n",
    "        ax1.set_title(\"ROC on validation set\")\n",
    "\n",
    "        ax2.plot(fpr, thresholds)\n",
    "        ax2.set_xlabel(\"FPR\")\n",
    "        ax2.set_ylabel(\"Thresholds\")\n",
    "        ax2.set_ylim(0.0, 1.01)\n",
    "\n",
    "        plt.axvline(x=0.005, color='black', ls=\":\", label=\"FPR=0.5%\")\n",
    "        plt.axvline(x=0.01, color='black', ls=\"--\", label=\"FPR=1%\")\n",
    "        writer.add_figure('ROC plot', plt.gcf(), global_step)\n",
    "        writer.flush()\n",
    "\n",
    "    \n",
    "    # Save the model at the end of every epoch\n",
    "    model_filename = f\"{save_dir}/{epoch}.pt\"\n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'global_step': global_step\n",
    "        }, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63657022-1b45-4bf7-947f-2b4855315171",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7655fbb2-a54d-4952-93ee-b9ccc146816a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c478019b-dc56-4045-b720-afef4dca2d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display image and label.\n",
    "valid_iterator = iter(valid_dataloader)\n",
    "valid_sample = next(valid_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "19c9926f-66aa-4b63-8347-b0023da86dc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 224, 224]), torch.Size([1]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_features, val_labels = valid_sample[\"image\"],  valid_sample[\"label\"]\n",
    "\n",
    "val_features.shape, val_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "137fe289-85f2-4411-ba3b-79fd58dbe0bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_output = model(val_features)\n",
    "val_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d6bac3c7-f2a4-4954-925f-f7d233716656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1322]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "64c2e50a-a00b-47e5-8ac7-c1a5787d5b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1322], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_output.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ca282a54-a4e3-4343-8112-cd1886263de6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "246676f4-313c-40e1-9f5c-f65891b2be05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_labels.type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "01d39811-20f8-4345-b1a8-45bf5e7484df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1418, grad_fn=<BinaryCrossEntropyBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss = loss_fn(val_output.view(-1), val_labels.type(torch.float))\n",
    "val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e672a35b-0377-4547-a252-9a4cc3a7d7b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ce2ddb18-6681-4fdb-8d57-a34d2fcf8419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0599, -0.1396]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3e0523-8b31-40fc-a6b5-e377897b5604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a787fb-1831-4b22-abde-6de0bb7f253f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb8346d-59f3-4f16-b4f5-60cc8e17914a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84916dfc-e4fb-428b-8235-b67a039e7c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b73978-73ad-4b8a-9ae6-6b5405d68d51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1692a7b4-797d-4deb-9f59-1e9b6934e522",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a34695cf-690d-4b1c-8914-7e952b600040",
   "metadata": {},
   "source": [
    "## TODO: Tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cd2c5d-b74a-41be-8b7f-1688a5595f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config():\n",
    "    return {\n",
    "        \"batch_size\": 8,\n",
    "        \"num_epochs\": 20,\n",
    "        \"lr\": 10**-4,\n",
    "        \"seq_len\": 350,\n",
    "        \"d_model\": 512,\n",
    "        \"datasource\": 'opus_books',\n",
    "        \"lang_src\": \"en\",\n",
    "        \"lang_tgt\": \"it\",\n",
    "        \"model_folder\": \"weights\",\n",
    "        \"model_basename\": \"tmodel_\",\n",
    "        \"preload\": \"latest\",\n",
    "        \"tokenizer_file\": \"tokenizer_{0}.json\",\n",
    "        \"experiment_name\": \"runs/tmodel\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2e692e31-6163-4d77-8101-b229a118acea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3]), torch.Size([3, 5]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "\n",
    "target.shape, input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00af4a85-a654-4a56-835c-4697989b7f0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a48de5e-e1a4-4864-8e4f-48cdee3e9467",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99598e5a-123b-4b0c-8cb2-be8b41ffba95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7785271b-8d89-4322-9da9-aa5d88d985b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1c3c42-4c09-4333-8a97-1e00639276cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba84e235-ca43-4fd9-b915-d170e6f6e2eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196ca67e-8773-42c0-99ef-72166cac5ffb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6653483-35e2-4e35-9a2a-0836222fb8de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a4a877-bf94-4829-8469-0f9f879de943",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbffb0fa-2452-4cab-ad88-756b81947f1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce2b6d5-f4ad-4bc6-8779-ea803bc067d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e01c06-734c-46d7-815a-d9c3b63685f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9567032e-09b3-4b58-a448-f152acd4edb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
